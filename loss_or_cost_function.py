# -*- coding: utf-8 -*-
"""Loss or cost function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CIpRu99cYHhPWh6JD57WZcn2UB-N3bQP

Loss is used to ensure & train NN training
"""



"""SUpervised Learning -> yhat = predicted output -> mean Absolute Error
for logistic regression we use log lost or Binary cross Entropy
"""

import numpy as np
y_predicted = np.array([1,1,0,0,1])
y_true = np.array([0,30,0,7,1,0,0.5])

def mae(y_true,y_predicted):
  total_error = 0
  for yt,yp in zip(y_true,y_predicted):
      total_error += abs(yt-yp)
  print("Total error : ",total_error)

  mae = total_error / len(y_true)
  print("MAE",mae)
  return mae

mae(y_true,y_predicted)



"""Total error = error^1 + error^2 + ...+ error^13
=∑ ^n i=1 abs(yi-y^i)  => Mean Absilute Error (MAE) = 1/n n∑i=1 abs(yi-y^i)
"""